{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'offline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-a82c1903eacf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_notebook_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotly\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'offline'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import pytz\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from nltk import bigrams as nltk_bigrams\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from __future__ import print_function\n",
    "from sklearn.feature_extraction import text\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename,num_tweets):\n",
    "    data = []\n",
    "    tweet_time = []\n",
    "    with open(filename,'r') as fp:\n",
    "        for index,line in tqdm(enumerate(fp),total=num_tweets):\n",
    "            entry = json.loads(line)\n",
    "            data.append(entry['title'])\n",
    "            tweet_time.append(datetime.datetime.fromtimestamp(entry['firstpost_date']))\n",
    "    return data,tweet_time\n",
    "\n",
    "def preprocessing(tweets):\n",
    "    snowball_stemmer = SnowballStemmer(\"english\")\n",
    "    processed_tweets = []\n",
    "    for tweet in tweets:\n",
    "        tweet = re.sub('[-.,></?;:(){}!$%^&*_=~`]', ' ', tweet)\n",
    "        tweet = ''.join(ch for ch in tweet if ch not in string.punctuation)\n",
    "        tweet = ''.join(ch for ch in tweet if ord(ch) < 128)  # remove non-ascii characters\n",
    "        words = [word for word in tweet.lower().split() \\\n",
    "                           if word not in text.ENGLISH_STOP_WORDS]\n",
    "        processed_tweets.append(' '.join(tweet))\n",
    "    return processed_tweets\n",
    "\n",
    "def get_data(precomputed=False):\n",
    "    csv_filename = os.path.join('data','train','part3','tweets.csv')\n",
    "    if(not precomputed):\n",
    "        if not os.path.exists(os.path.join('data','train','part3')):\n",
    "            os.makedirs(os.path.join('data','train','part3'))\n",
    "        filenames = {\n",
    "            'tweets_#gohawks' : 188136,\n",
    "            'tweets_#nfl' : 259024,\n",
    "            'tweets_#sb49' : 826951,\n",
    "            'tweets_#gopatriots' : 26232,\n",
    "            'tweets_#patriots' : 489713,\n",
    "            'tweets_#superbowl' : 1348767\n",
    "        }\n",
    "        tweet=[]\n",
    "        tweet_time=[]\n",
    "        for key,num in filenames.items():\n",
    "            print('Loading',key,'....')\n",
    "            sub_tweets, sub_time = read_data(os.path.join('data','train',key+'.txt'),num)\n",
    "            tweet += sub_tweets\n",
    "            tweet_time += sub_time\n",
    "        dataset = pd.DataFrame({'tweet_time':tweet_time,'tweet':tweet})\n",
    "        dataset.to_csv(csv_filename,encoding='utf-8',index=False)\n",
    "    else:\n",
    "        dataset = pd.read_csv(csv_filename,encoding='utf-8',engine='python')\n",
    "        dataset['tweet_time'].replace(u'', np.nan, inplace=True)\n",
    "        dataset.dropna(subset=['tweet_time'], inplace=True)\n",
    "        dataset['tweet_time'] = dataset['tweet_time'].map(lambda x: datetime.datetime.strptime(str(x),\"%Y-%m-%d %H:%M:%S\"))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweets_#patriots ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 489713/489713 [04:08<00:00, 1971.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweets_#gopatriots ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 26232/26232 [00:13<00:00, 2007.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweets_#superbowl ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 1348767/1348767 [10:55<00:00, 2057.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweets_#nfl ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 259024/259024 [02:48<00:00, 1532.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweets_#gohawks ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 188136/188136 [01:35<00:00, 1967.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweets_#sb49 ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 826951/826951 [07:04<00:00, 1947.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets = 3138823\n"
     ]
    }
   ],
   "source": [
    "df = get_data(precomputed=False)\n",
    "print('Number of tweets =',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_period_start = datetime.datetime(2015,2,1,14,0,0)\n",
    "int_period_end = datetime.datetime(2015,2,1,21,0,0)\n",
    "df = df[df.tweet_time.apply(lambda x : x > int_period_start)]\n",
    "df = df[df.tweet_time.apply(lambda x : x < int_period_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = df.set_index('tweet_time').groupby(pd.TimeGrouper(freq='15Min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(group_tweets):\n",
    "    all_hashtags = Counter()\n",
    "    bigrams_counter = Counter()\n",
    "    all_words = Counter() # words without #hashtags and @mentions\n",
    "    hash_tag_str = re.compile(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\")\n",
    "    for tweet in group_tweets:\n",
    "        hashtags = map(lambda x : str(x.lower()),hash_tag_str.findall(tweet))\n",
    "        all_hashtags.update(hashtags)\n",
    "        tweet = re.sub('[,.-:/()?{}*$&]', ' ', tweet)\n",
    "        tweet = ''.join(ch for ch in tweet if ch not in string.punctuation)\n",
    "        tweet = ''.join(ch for ch in tweet if ord(ch) < 128)  # remove non-ascii characters\n",
    "        words = [str(word) for word in tweet.lower().split() \\\n",
    "                           if word not in text.ENGLISH_STOP_WORDS]\n",
    "\n",
    "        regular_words = [w for w in words if not w.startswith(('#', '@'))]\n",
    "        all_words.update(regular_words)\n",
    "        bigrams_counter.update(nltk_bigrams(regular_words))\n",
    "\n",
    "    return all_hashtags, all_words, bigrams_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trending Ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Advertisement():\n",
    "    def __init__(self,company,taglines,bigram_name=''):\n",
    "        self.name = company\n",
    "        self.text = re.sub('[- \\']','',company).lower()\n",
    "        self.taglines = self.filterTaglines(taglines)\n",
    "        self.bigram_name = bigram_name.lower()\n",
    "        self.count=0\n",
    "        self.timeframe_count = []\n",
    "        \n",
    "    def filterTaglines(self,taglines):\n",
    "        filtered_tag = []\n",
    "        for tag in taglines:\n",
    "            tag = re.sub(\"[-.,\\'></?;:#(){}!$%^&*_=~ `]\", '', tag)\n",
    "            tag = ''.join(ch for ch in tag if ch not in string.punctuation)\n",
    "            tag = '#'+tag\n",
    "            filtered_tag.append(tag.lower())\n",
    "        return filtered_tag\n",
    "    \n",
    "    def addTagLine(self,tagline):\n",
    "        self.taglines.append(tagline)\n",
    "        \n",
    "    def updateCount(self,value):\n",
    "        self.count+=value\n",
    "    \n",
    "    def updateTimeFrameCount(self):\n",
    "        self.timeframe_count.append(self.count)\n",
    "        self.count = 0\n",
    "\n",
    "def create_adclasses():\n",
    "    tmobile = Advertisement('T-Mobile',['One Upped','#Kim\\'s Data Stash'])\n",
    "    budweiser = Advertisement('Budweiser',['Lost Dog','Brewed the Hard Way'])\n",
    "    bmw = Advertisement('BMW',['Newfangled Idea'])\n",
    "    cocacola = Advertisement('Coca Cola',['Make It Happy'],'Coca Cola')\n",
    "    doritos = Advertisement('Doritos',['When Pigs Fly','Middle Seat'])\n",
    "    esurance = Advertisement('Esurance',['Sorta Pharmacist'])\n",
    "    loctite = Advertisement('Loctite',['Positive Feelings'])\n",
    "    mcd = Advertisement('McDonald\\'s',['Pay With Lovin'],'McDonald s')\n",
    "    snickers = Advertisement('Snickers',['Very Brady'])\n",
    "    toyota = Advertisement('Toyota',['How Great I Am','My Great Dad'])\n",
    "    return [\\\n",
    "            tmobile,budweiser,\\\n",
    "            bmw,cocacola,\\\n",
    "            doritos,esurance,\\\n",
    "            loctite,mcd,\\\n",
    "            snickers,toyota\\\n",
    "           ]\n",
    "\n",
    "class Person():\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.fname = name.split()[0].lower()\n",
    "        self.lname = name.split()[1].lower()\n",
    "        self.count=0\n",
    "        self.timeframe_count = []\n",
    "\n",
    "    def updateCount(self,value):\n",
    "        self.count+=value\n",
    "    \n",
    "    def updateTimeFrameCount(self):\n",
    "        self.timeframe_count.append(self.count)\n",
    "        self.count = 0\n",
    "        \n",
    "def create_celebclasses():\n",
    "    celebs = ['Mark Wahlberg','Katy Perry','Lenny Kravitz',\n",
    "              'Missy Elliott','Idina Menzel','Kevin Hart']\n",
    "    classes = []\n",
    "    for p in celebs:\n",
    "        classes.append(Person(p))\n",
    "\n",
    "    return classes\n",
    "\n",
    "def create_hawksclasses():\n",
    "    players = ['Russell Wilson','Jermaine Kearse','Marshawn Lynch',\n",
    "              'Steven Hauschka','KJ Wright','Bobby Wagner',\n",
    "               'Ricardo Lockette','Earl Thomas','Kam Chancellor',\n",
    "               'Chris Matthews']\n",
    "    classes = []\n",
    "    for p in players:\n",
    "        classes.append(Person(p))\n",
    "\n",
    "    return classes\n",
    "\n",
    "def create_patriotsclasses():\n",
    "    players = ['Tom Brady','Rob Gronkowski','Brandon LaFell',\n",
    "               'Julian Edelman','Shane Vereen','Stephen Gostkowski',\n",
    "               'Danny Amendola','Malcolm Butler','Jamie Collins',\n",
    "               'LeGarrette Blount']\n",
    "    classes = []\n",
    "    for p in players:\n",
    "        classes.append(Person(p))\n",
    "\n",
    "    return classes\n",
    "\n",
    "def get_ad_information(companies,hashtags,keywords,bigram_words):\n",
    "    total_ad_count = 0\n",
    "    for ad in companies:\n",
    "        for count, word in enumerate(keywords.keys()):\n",
    "            if word == ad.text:\n",
    "                count = keywords.get(word)\n",
    "                ad.updateCount(count)\n",
    "                total_ad_count += count\n",
    "\n",
    "        for count, htag in enumerate(hashtags.keys()):\n",
    "            if ad.text in htag or htag == ad.taglines:\n",
    "                count = hashtags.get(htag)\n",
    "                ad.updateCount(count)\n",
    "                total_ad_count += count\n",
    "\n",
    "        for count, bg_pair in enumerate(bigram_words.keys()):\n",
    "            bg_word = ' '.join(x for x in bg_pair)\n",
    "            try:\n",
    "                if bg_word in ad.bigram_name:\n",
    "                    count = bigram_words.get(bg_pair)\n",
    "                    ad.updateCount(count)\n",
    "                    total_ad_count += count\n",
    "            except:\n",
    "                print(\"BG_WORD\",bg_word)\n",
    "        ad.updateTimeFrameCount()\n",
    "    return total_ad_count\n",
    "\n",
    "def get_person_information(persons, hash_tags, key_words):\n",
    "    total_person_count = 0\n",
    "    for p in persons:\n",
    "        for count, word in enumerate(key_words.keys()):\n",
    "            if word == p.fname or word == p.lname:\n",
    "                count = keywords.get(word)\n",
    "                p.updateCount(count)\n",
    "                total_person_count += count\n",
    "                \n",
    "        for count, htag in enumerate(hash_tags.keys()):\n",
    "            #Checking like this because lname can be Brady, and htag can be #BradyIsKillingIt\n",
    "            if p.fname in htag or p.lname in htag:\n",
    "                count = hash_tags.get(htag)\n",
    "                p.updateCount(count)\n",
    "                total_person_count += count        \n",
    "        p.updateTimeFrameCount()\n",
    "    return total_person_count\n",
    "\n",
    "def get_timeseries(entities,timestamps,normalize):\n",
    "    length = len(entities[0].timeframe_count)\n",
    "    ads = pd.DataFrame({'time':timestamps[:length]})\n",
    "    for entity in entities:\n",
    "        ads[entity.name] = entity.timeframe_count\n",
    "    ads = ads.set_index('time')\n",
    "    if(normalize=='columns'):\n",
    "        ads = (ads-ads.mean())/(ads.max()-ads.min())  # Normalizing the columns\n",
    "        ads[ads<0] = 0\n",
    "    if(normalize=='rows'):\n",
    "        ads = ads.div(ads.sum(axis=1), axis=0)\n",
    "        ads[ads<0] = 0\n",
    "    match_data = dict(ads) \n",
    "    all_matches = pd.DataFrame(match_data)\n",
    "    all_matches[all_matches < 0] = 0\n",
    "    return all_matches\n",
    "\n",
    "def plot_graph(results,filename):\n",
    "    lines = []\n",
    "    for name in results.columns:\n",
    "        trace = go.Scatter(\n",
    "            y = results[name],\n",
    "            x = result.index,\n",
    "            mode = 'lines+markers',\n",
    "            name = name\n",
    "        )\n",
    "        lines.append(trace)\n",
    "    layout = go.Layout(\n",
    "        xaxis=dict(title='Time'),\n",
    "        yaxis=dict(title='Tweets')\n",
    "    )\n",
    "    fig = go.Figure(data=lines, layout=layout)\n",
    "    py.iplot(fig, filename=filename)\n",
    "    py.image.save_as(fig, filename=os.path.join('graphs',filename+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-02-01 14:00:00\n",
      "57 380 405 942\n",
      "2015-02-01 14:15:00\n",
      "48 354 438 883\n",
      "2015-02-01 14:30:00\n",
      "96 425 614 1271\n",
      "2015-02-01 14:45:00\n",
      "105 979 825 1631\n",
      "2015-02-01 15:00:00\n",
      "187 1655 1691 4661\n",
      "2015-02-01 15:15:00\n",
      "453 7648 1915 3936\n",
      "2015-02-01 15:30:00\n",
      "2159 2367 2887 4449\n",
      "2015-02-01 15:45:00\n",
      "5162 2426 2059 12391\n",
      "2015-02-01 16:00:00\n",
      "11888 2305 1826 10675\n",
      "2015-02-01 16:15:00\n",
      "6405 2782 2537 6380\n",
      "2015-02-01 16:30:00\n",
      "8685 4319 9260 2262\n",
      "2015-02-01 16:45:00\n",
      "1810 5259 3961 9347\n",
      "2015-02-01 17:00:00\n",
      "2117 31006 5128 2367\n",
      "2015-02-01 17:15:00\n",
      "467 100894 860 1013\n",
      "2015-02-01 17:30:00\n",
      "703 23014 9595 1516\n",
      "2015-02-01 17:45:00\n",
      "1433 7981 9212 7859\n",
      "2015-02-01 18:00:00\n",
      "2001 5259 2670 3456\n",
      "2015-02-01 18:15:00\n",
      "3124 4049 2439 8942\n",
      "2015-02-01 18:30:00\n",
      "2386 3121 1314 6846\n",
      "2015-02-01 18:45:00\n",
      "460 2156 5329 12335\n",
      "2015-02-01 19:00:00\n",
      "227 1438 6336 14641\n",
      "2015-02-01 19:15:00\n",
      "234 1542 2647 10697\n",
      "2015-02-01 19:30:00\n",
      "159 1423 1583 5753\n",
      "2015-02-01 19:45:00\n",
      "76 679 722 2447\n"
     ]
    }
   ],
   "source": [
    "companies = create_adclasses()\n",
    "celebs = create_celebclasses()\n",
    "seahawks = create_hawksclasses()\n",
    "patriots = create_patriotsclasses()\n",
    "for i, group in sorted(grouped_data):\n",
    "    print(i)\n",
    "    hashtags,keywords,bigram_words = preprocess_data(group.tweet)\n",
    "    ads_count = get_ad_information(companies,hashtags,keywords,bigram_words)\n",
    "    celeb_count = get_person_information(celebs,hashtags,keywords)\n",
    "    seahawks_count = get_person_information(seahawks,hashtags,keywords)\n",
    "    patriots_count = get_person_information(patriots,hashtags,keywords)\n",
    "    print(ads_count,celeb_count,seahawks_count,patriots_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trending Ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_timeseries(companies,sorted(grouped_data.groups.keys()),normalize=None)\n",
    "plot_graph(result,'ad-all')\n",
    "result2 = get_timeseries(companies,sorted(grouped_data.groups.keys()),normalize='columns')\n",
    "plot_graph(result2,'ad-normalized-column')\n",
    "result3 = get_timeseries(companies,sorted(grouped_data.groups.keys()),normalize='rows')\n",
    "plot_graph(result3,'ad-normalized-rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trending Celebrities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_timeseries(celebs,sorted(grouped_data.groups.keys()),normalize=None)\n",
    "plot_graph(result,'celebs-all')\n",
    "result2 = get_timeseries(celebs,sorted(grouped_data.groups.keys()),normalize='columns')\n",
    "plot_graph(result2,'celebs-normalized-column')\n",
    "result3 = get_timeseries(celebs,sorted(grouped_data.groups.keys()),normalize='rows')\n",
    "plot_graph(result3,'celebs-normalized-rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trending Players - Seattle Seahawks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_timeseries(seahawks,sorted(grouped_data.groups.keys()),normalize=None)\n",
    "plot_graph(result,'seahawks-all')\n",
    "result2 = get_timeseries(seahawks,sorted(grouped_data.groups.keys()),normalize='columns')\n",
    "plot_graph(result2,'seahawks-normalized-column')\n",
    "result3 = get_timeseries(seahawks,sorted(grouped_data.groups.keys()),normalize='rows')\n",
    "plot_graph(result3,'seahawks-normalized-rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trending Players - New England Patriots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_timeseries(patriots,sorted(grouped_data.groups.keys()),normalize=None)\n",
    "plot_graph(result,'patriots-all')\n",
    "result2 = get_timeseries(patriots,sorted(grouped_data.groups.keys()),normalize='columns')\n",
    "plot_graph(result2,'patriots-normalized-column')\n",
    "result3 = get_timeseries(patriots,sorted(grouped_data.groups.keys()),normalize='rows')\n",
    "plot_graph(result3,'patriots-normalized-rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
