{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import pytz\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from nltk import bigrams as nltk_bigrams\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from __future__ import print_function\n",
    "from sklearn.feature_extraction import text\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename,num_tweets):\n",
    "    data = []\n",
    "    tweet_time = []\n",
    "    with open(filename,'r') as fp:\n",
    "        for index,line in tqdm(enumerate(fp),total=num_tweets):\n",
    "            entry = json.loads(line)\n",
    "            data.append(entry['title'])\n",
    "            tweet_time.append(datetime.datetime.fromtimestamp(entry['firstpost_date']))\n",
    "    return data,tweet_time\n",
    "\n",
    "def preprocessing(tweets):\n",
    "    snowball_stemmer = SnowballStemmer(\"english\")\n",
    "    processed_tweets = []\n",
    "    for tweet in tweets:\n",
    "        tweet = re.sub('[-.,></?;:(){}!$%^&*_=~`]', ' ', tweet)\n",
    "        tweet = ''.join(ch for ch in tweet if ch not in string.punctuation)\n",
    "        tweet = ''.join(ch for ch in tweet if ord(ch) < 128)  # remove non-ascii characters\n",
    "        words = [word for word in tweet.lower().split() \\\n",
    "                           if word not in text.ENGLISH_STOP_WORDS]\n",
    "        processed_tweets.append(' '.join(tweet))\n",
    "    return processed_tweets\n",
    "\n",
    "def get_data(precomputed=False):\n",
    "    csv_filename = os.path.join('data','train','part3','tweets.csv')\n",
    "    if(not precomputed):\n",
    "        if not os.path.exists(os.path.join('data','train','part3')):\n",
    "            os.makedirs(os.path.join('data','train','part3'))\n",
    "        filenames = {\n",
    "            #'tweets_#gohawks' : 188136,\n",
    "            #'tweets_#nfl' : 259024,\n",
    "            #'tweets_#sb49' : 826951,\n",
    "            #'tweets_#gopatriots' : 26232,\n",
    "            #'tweets_#patriots' : 489713,\n",
    "            'tweets_#superbowl' : 1348767\n",
    "        }\n",
    "        tweet=[]\n",
    "        tweet_time=[]\n",
    "        for key,num in filenames.items():\n",
    "            print('Loading',key,'....')\n",
    "            sub_tweets, sub_time = read_data(os.path.join('data','train',key+'.txt'),num)\n",
    "            tweet += sub_tweets\n",
    "            tweet_time += sub_time\n",
    "        dataset = pd.DataFrame({'tweet_time':tweet_time,'tweet':tweet})\n",
    "        dataset.to_csv(csv_filename,encoding='utf-8',index=False)\n",
    "    else:\n",
    "        dataset = pd.read_csv(csv_filename,encoding='utf-8',engine='python')\n",
    "        dataset['tweet_time'].replace(u'', np.nan, inplace=True)\n",
    "        dataset.dropna(subset=['tweet_time'], inplace=True)\n",
    "        dataset['tweet_time'] = dataset['tweet_time'].map(lambda x: datetime.datetime.strptime(str(x),\"%Y-%m-%d %H:%M:%S\"))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweets_#superbowl ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 1348767/1348767 [10:39<00:00, 2110.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets = 1348767\n"
     ]
    }
   ],
   "source": [
    "df = get_data(precomputed=True)\n",
    "print('Number of tweets =',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_period_start = datetime.datetime(2015,2,1,14,0,0)\n",
    "int_period_end = datetime.datetime(2015,2,1,20,0,0)\n",
    "df = df[df.tweet_time.apply(lambda x : x > int_period_start)]\n",
    "df = df[df.tweet_time.apply(lambda x : x < int_period_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = df.set_index('tweet_time').groupby(pd.TimeGrouper(freq='10Min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(group_tweets):\n",
    "    all_hashtags = Counter()\n",
    "    bigrams_counter = Counter()\n",
    "    all_words = Counter() # words without #hashtags and @mentions\n",
    "    hash_tag_str = re.compile(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\")\n",
    "    for tweet in group_tweets:\n",
    "        hashtags = map(lambda x : str(x.lower()),hash_tag_str.findall(tweet))\n",
    "        all_hashtags.update(hashtags)\n",
    "        tweet = re.sub('[,.-:/()?{}*$&]', ' ', tweet)\n",
    "        tweet = ''.join(ch for ch in tweet if ch not in string.punctuation)\n",
    "        tweet = ''.join(ch for ch in tweet if ord(ch) < 128)  # remove non-ascii characters\n",
    "        words = [str(word) for word in tweet.lower().split() \\\n",
    "                           if word not in text.ENGLISH_STOP_WORDS]\n",
    "\n",
    "        regular_words = [w for w in words if not w.startswith(('#', '@'))]\n",
    "        all_words.update(regular_words)\n",
    "        bigrams_counter.update(nltk_bigrams(regular_words))\n",
    "\n",
    "    return all_hashtags, all_words, bigrams_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trending Ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Advertisement():\n",
    "    def __init__(self,company,taglines,bigram_name=''):\n",
    "        self.company = company\n",
    "        self.name = re.sub('[- \\']','',company).lower()\n",
    "        self.taglines = self.filterTaglines(taglines)\n",
    "        self.bigram_name = bigram_name.lower()\n",
    "        self.count=0\n",
    "        self.timeframe_count = []\n",
    "        \n",
    "    def filterTaglines(self,taglines):\n",
    "        filtered_tag = []\n",
    "        for tag in taglines:\n",
    "            tag = re.sub(\"[-.,\\'></?;:#(){}!$%^&*_=~ `]\", '', tag)\n",
    "            tag = ''.join(ch for ch in tag if ch not in string.punctuation)\n",
    "            tag = '#'+tag\n",
    "            filtered_tag.append(tag.lower())\n",
    "        return filtered_tag\n",
    "    \n",
    "    def addTagLine(self,tagline):\n",
    "        self.taglines.append(tagline)\n",
    "        \n",
    "    def updateCount(self,value):\n",
    "        self.count+=value\n",
    "    \n",
    "    def updateTimeFrameCount(self):\n",
    "        self.timeframe_count.append(self.count)\n",
    "        self.count = 0\n",
    "\n",
    "def create_adclasses():\n",
    "    tmobile = Advertisement('T-Mobile',['One Upped','#Kim\\'s Data Stash'])\n",
    "    budweiser = Advertisement('Budweiser',['Lost Dog','Brewed the Hard Way'])\n",
    "    bmw = Advertisement('BMW',['Newfangled Idea'])\n",
    "    cocacola = Advertisement('Coca Cola',['Make It Happy'],'Coca Cola')\n",
    "    doritos = Advertisement('Doritos',['When Pigs Fly','Middle Seat'])\n",
    "    esurance = Advertisement('Esurance',['Sorta Pharmacist'])\n",
    "    loctite = Advertisement('Loctite',['Positive Feelings'])\n",
    "    mcd = Advertisement('McDonald\\'s',['Pay With Lovin'],'McDonald s')\n",
    "    snickers = Advertisement('Snickers',['Very Brady'])\n",
    "    toyota = Advertisement('Toyota',['How Great I Am','My Great Dad'])\n",
    "    victoria = Advertisement('Victoria\\'s Secret',['Let the Real Games Begin'],'Victoria s Secret')\n",
    "    return [\\\n",
    "            tmobile,budweiser,\\\n",
    "            bmw,cocacola,\\\n",
    "            doritos,esurance,\\\n",
    "            loctite,mcd,\\\n",
    "            snickers,supercell,\\\n",
    "            toyota,victoria\\\n",
    "           ]\n",
    "\n",
    "def get_ad_information(companies,hashtags,keywords,bigram_words):\n",
    "    total_ad_count = 0\n",
    "    for ad in companies:\n",
    "        for count, word in enumerate(keywords.keys()):\n",
    "            if word == ad.name:\n",
    "                count = keywords.get(word)\n",
    "                ad.updateCount(count)\n",
    "                total_ad_count += count\n",
    "\n",
    "        for count, htag in enumerate(hashtags.keys()):\n",
    "            if ad.name in htag or htag == ad.taglines:\n",
    "                count = hashtags.get(htag)\n",
    "                ad.updateCount(count)\n",
    "                total_ad_count += count\n",
    "\n",
    "        for count, bg_pair in enumerate(bigram_words.keys()):\n",
    "            bg_word = ' '.join(x for x in bg_pair)\n",
    "            if bg_word in ad.bigram_name:\n",
    "                count = bigram_words.get(bg_pair)\n",
    "                ad.updateCount(count)\n",
    "                total_ad_count += count\n",
    "        ad.updateTimeFrameCount()\n",
    "    return total_ad_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "24\n",
      "11\n",
      "28\n",
      "29\n",
      "43\n",
      "87\n",
      "87\n",
      "259\n",
      "804\n"
     ]
    }
   ],
   "source": [
    "companies = create_adclasses()\n",
    "count=0\n",
    "for i, group in grouped_data:\n",
    "    #print(group.tweet)\n",
    "    hashtags,keywords,bigram_words = preprocess_data(group.tweet)\n",
    "    #break\n",
    "    ads_count = get_ad_information(companies,hashtags,keywords,bigram_words)\n",
    "    count+=1\n",
    "    print(ads_count)\n",
    "    if(count==10):\n",
    "        break\n",
    "    #celeb_df = get_celebrities(hashtags,keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [0, 2, 0, 0, 1, 0, 0, 0, 11, 7]\n",
      " [3, 7, 5, 10, 13, 14, 38, 30, 29, 22]\n",
      " [2, 2, 0, 0, 7, 6, 8, 5, 6, 0]\n",
      "Coca Cola [0, 0, 0, 4, 0, 1, 3, 11, 7, 8]\n",
      " [4, 8, 4, 8, 8, 13, 15, 21, 29, 35]\n",
      " [0, 0, 0, 0, 0, 1, 1, 0, 17, 401]\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "McDonald s [4, 0, 0, 2, 0, 2, 7, 10, 148, 24]\n",
      " [1, 0, 0, 4, 0, 6, 6, 1, 7, 8]\n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " [0, 1, 0, 0, 0, 0, 3, 7, 3, 293]\n",
      "Victoria s Secret [1, 4, 2, 0, 0, 0, 6, 2, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "for ad in companies:\n",
    "    print(ad.bigram_name,ad.timeframe_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trending Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trending Celebrities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_first = [\"John\", \"Idina\", \"Katy\", \"Lenny\", \"Missy\", \"Nina\", \"Josh\"]\n",
    "celeb_last = [\"Legend\", \"Menzel\", \"Perry\", \"Kravitz\", \"Elliott\", \"Dobrev\", \"Duhamel\"]\n",
    "\n",
    "def get_celebrities(hash_tags, key_words):\n",
    "    local_celeb_count = 0\n",
    "    celeb_count = np.zeros(len(celeb_first))\n",
    "\n",
    "    for count, tweet in enumerate(key_words.keys()):\n",
    "        for i in range(len(celeb_count)):\n",
    "            if tweet.find(celeb_first[i].lower()) > -1 or tweet.find(celeb_last[i].lower()) > -1:\n",
    "                celeb_count[i] += key_words.get(tweet)\n",
    "                local_celeb_count += key_words.get(tweet)\n",
    "\n",
    "    for count, tweet in enumerate(hash_tags.keys()):\n",
    "        for i in range(len(celeb_count)):\n",
    "            if tweet.find(celeb_first[i].lower()) > -1 or tweet.find(celeb_last[i].lower()) > -1:\n",
    "                celeb_count[i] += hash_tags.get(tweet)\n",
    "                local_celeb_count += hash_tags.get(tweet)\n",
    "\n",
    "    hour_status.append(celeb_count)\n",
    "    return local_celeb_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
